{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZio0p+1G2c4iG+gF6KL0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msf1997/taiyo/blob/main/Assignment_Data_Engineer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9GXdWBFKgVHB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class WorldBankTendersScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def scrape(self):\n",
        "        response = requests.get(self.url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            tender_items = soup.find_all('div', class_='views-row')\n",
        "\n",
        "            data = []\n",
        "            for item in tender_items:\n",
        "                title_elem = item.find('div', class_='views-field-title')\n",
        "                title = title_elem.get_text().strip() if title_elem else \"N/A\"\n",
        "\n",
        "                description_elem = item.find('div', class_='views-field-field-pd-description')\n",
        "                description = description_elem.get_text().strip() if description_elem else \"N/A\"\n",
        "\n",
        "                project_elem = item.find('div', class_='views-field-field-pd-project')\n",
        "                project = project_elem.get_text().strip() if project_elem else \"N/A\"\n",
        "\n",
        "                country_elem = item.find('div', class_='views-field-field-pd-country')\n",
        "                country = country_elem.get_text().strip() if country_elem else \"N/A\"\n",
        "\n",
        "                data.append({\n",
        "                    'Title': title,\n",
        "                    'Description': description,\n",
        "                    'Project': project,\n",
        "                    'Country': country\n",
        "                })\n",
        "\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data from {self.url}\")\n",
        "            return []\n",
        "\n",
        "    def save_to_csv(self, data):\n",
        "        if data:\n",
        "            with open('world_bank_tenders.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['Title', 'Description', 'Project', 'Country']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for row in data:\n",
        "                    writer.writerow(row)\n",
        "            print(\"Data saved to world_bank_tenders.csv\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    url = 'https://ieg.worldbankgroup.org/data'\n",
        "    scraper = WorldBankTendersScraper(url)\n",
        "    scraped_data = scraper.scrape()\n",
        "    scraper.save_to_csv(scraped_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzElJHo8gvbR",
        "outputId": "ce9b5fc8-91c1-4842-dbe5-4d4819f212f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to world_bank_tenders.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "class ChinaBiddingScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def scrape(self):\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "        }\n",
        "\n",
        "        response = requests.get(self.url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            tender_items = soup.find_all('div', class_='content-box')\n",
        "\n",
        "            data = []\n",
        "            for item in tender_items:\n",
        "                title_elem = item.find('h3', class_='news-hd')\n",
        "                title = title_elem.get_text().strip() if title_elem else \"N/A\"\n",
        "\n",
        "                date_elem = item.find('span', class_='time')\n",
        "                date = date_elem.get_text().strip() if date_elem else \"N/A\"\n",
        "\n",
        "                data.append({\n",
        "                    'Title': title,\n",
        "                    'Date': date\n",
        "                })\n",
        "\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data from {self.url}\")\n",
        "            return []\n",
        "\n",
        "    def save_to_csv(self, data):\n",
        "        if data:\n",
        "            with open('china_bidding_tenders.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['Title', 'Date']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for row in data:\n",
        "                    writer.writerow(row)\n",
        "        print(\"Data saved to china_bidding_tenders.csv\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    url = 'https://www.chinabidding.com/en'\n",
        "    scraper = ChinaBiddingScraper(url)\n",
        "    scraped_data = scraper.scrape()\n",
        "    scraper.save_to_csv(scraped_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2Y4xrRehEfD",
        "outputId": "bdd9ad9d-6b5c-470f-e586-1eb8d98e8c33"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to china_bidding_tenders.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ChinabiddingMOFCOMScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def scrape(self):\n",
        "        response = requests.get(self.url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            tender_items = soup.find_all('li', class_='li2')\n",
        "\n",
        "            data = []\n",
        "            for item in tender_items:\n",
        "                title_elem = item.find('a', class_='mmu1')\n",
        "                title = title_elem.get_text().strip() if title_elem else \"N/A\"\n",
        "\n",
        "                date_elem = item.find('span', class_='span5')\n",
        "                date = date_elem.get_text().strip() if date_elem else \"N/A\"\n",
        "\n",
        "                data.append({\n",
        "                    'Title': title,\n",
        "                    'Date': date\n",
        "                })\n",
        "\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data from {self.url}\")\n",
        "            return []\n",
        "\n",
        "    def save_to_csv(self, data):\n",
        "        if data:\n",
        "            with open('chinabidding_mofcom_tenders.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['Title', 'Date']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for row in data:\n",
        "                    writer.writerow(row)\n",
        "        print(\"Data saved to chinabidding_mofcom_tenders.csv\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    url = 'http://en.chinabidding.mofcom.gov.cn/'\n",
        "    scraper = ChinabiddingMOFCOMScraper(url)\n",
        "    scraped_data = scraper.scrape()\n",
        "    scraper.save_to_csv(scraped_data)\n"
      ],
      "metadata": {
        "id": "5jCpBPDCXddz",
        "outputId": "b3d49ebb-cb8e-487b-8b5d-7de8ebdb0f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to chinabidding_mofcom_tenders.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CPPPCScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def scrape(self):\n",
        "        response = requests.get(self.url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            tender_items = soup.find_all('div', class_='search_main_item')\n",
        "\n",
        "            data = []\n",
        "            for item in tender_items:\n",
        "                title_elem = item.find('a', class_='search_main_item_ltitle')\n",
        "                title = title_elem.get_text().strip() if title_elem else \"N/A\"\n",
        "\n",
        "                date_elem = item.find('span', class_='search_main_item_ldate')\n",
        "                date = date_elem.get_text().strip() if date_elem else \"N/A\"\n",
        "\n",
        "                data.append({\n",
        "                    'Title': title,\n",
        "                    'Date': date\n",
        "                })\n",
        "\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data from {self.url}\")\n",
        "            return []\n",
        "\n",
        "    def save_to_csv(self, data):\n",
        "        if data:\n",
        "            with open('cpppc_tenders.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['Title', 'Date']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for row in data:\n",
        "                    writer.writerow(row)\n",
        "        print(\"Data saved to cpppc_tenders.csv\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    url = 'https://www.cpppc.org/en/PPPyd.jhtml'\n",
        "    scraper = CPPPCScraper(url)\n",
        "    scraped_data = scraper.scrape()\n",
        "    scraper.save_to_csv(scraped_data)\n"
      ],
      "metadata": {
        "id": "-bViay_eh1TS",
        "outputId": "8534c069-74e6-4afb-d903-217dfdcb30e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to cpppc_tenders.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CPPPC8082Scraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def scrape(self):\n",
        "        response = requests.get(self.url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            tender_items = soup.find_all('div', class_='search_item')\n",
        "\n",
        "            data = []\n",
        "            for item in tender_items:\n",
        "                title_elem = item.find('a', class_='search_item_title')\n",
        "                title = title_elem.get_text().strip() if title_elem else \"N/A\"\n",
        "\n",
        "                date_elem = item.find('div', class_='search_item_date')\n",
        "                date = date_elem.get_text().strip() if date_elem else \"N/A\"\n",
        "\n",
        "                data.append({\n",
        "                    'Title': title,\n",
        "                    'Date': date\n",
        "                })\n",
        "\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data from {self.url}\")\n",
        "            return []\n",
        "\n",
        "    def save_to_csv(self, data):\n",
        "        if data:\n",
        "            with open('cpppc_8082_tenders.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['Title', 'Date']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for row in data:\n",
        "                    writer.writerow(row)\n",
        "        print(\"Data saved to cpppc_8082_tenders.csv\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    url = 'https://www.cpppc.org:8082/inforpublic/homepage.html#/searchresult'\n",
        "    scraper = CPPPC8082Scraper(url)\n",
        "    scraped_data = scraper.scrape()\n",
        "    scraper.save_to_csv(scraped_data)\n"
      ],
      "metadata": {
        "id": "NJHh0KhojcKL",
        "outputId": "99640f7d-3032-404b-f6e2-52966c5e574f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve data from https://www.cpppc.org:8082/inforpublic/homepage.html#/searchresult\n",
            "Data saved to cpppc_8082_tenders.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E-procurement Government of India:**"
      ],
      "metadata": {
        "id": "3Q12_sg1kPf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ETendersScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def scrape(self):\n",
        "        response = requests.get(self.url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            tender_items = soup.find_all('div', class_='table-main-row')\n",
        "\n",
        "            data = []\n",
        "            for item in tender_items:\n",
        "                title_elem = item.find('div', class_='col-md-6 col-xs-12')\n",
        "                title = title_elem.get_text().strip() if title_elem else \"N/A\"\n",
        "\n",
        "                date_elem = item.find('div', class_='col-md-3 col-xs-6 text-right')\n",
        "                date = date_elem.get_text().strip() if date_elem else \"N/A\"\n",
        "\n",
        "                data.append({\n",
        "                    'Title': title,\n",
        "                    'Date': date\n",
        "                })\n",
        "\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data from {self.url}\")\n",
        "            return []\n",
        "\n",
        "    def save_to_csv(self, data):\n",
        "        if data:\n",
        "            with open('etenders_tenders.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['Title', 'Date']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for row in data:\n",
        "                    writer.writerow(row)\n",
        "        print(\"Data saved to etenders_tenders.csv\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    url = 'https://etenders.gov.in/eprocure/app'\n",
        "    scraper = ETendersScraper(url)\n",
        "    scraped_data = scraper.scrape()\n",
        "    scraper.save_to_csv(scraped_data)\n"
      ],
      "metadata": {
        "id": "sh2QePlakSqK",
        "outputId": "ef33fbd4-7441-4803-e3fb-37b95632090c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to etenders_tenders.csv\n"
          ]
        }
      ]
    }
  ]
}